{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StyleGAN Practical Lab: Image Editing and Latent Space Manipulation\n",
    "\n",
    "## Lab Objectives\n",
    "\n",
    "- Understand the basic functioning of StyleGAN\n",
    "- Generate realistic images using pre-trained models\n",
    "- Project real images to latent space\n",
    "- Find semantic directions in latent space\n",
    "- Apply directed edits to images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Setup and Dependencies\n",
    "\n",
    "### Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install torch torchvision\n",
    "!pip install ninja\n",
    "!pip install requests pillow numpy matplotlib\n",
    "!pip install opencv-python\n",
    "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
    "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3\n",
    "\n",
    "import os\n",
    "os.chdir('/content/stylegan2-ada-pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "import io\n",
    "from google.colab import files\n",
    "import cv2\n",
    "\n",
    "# Configure device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# CPU Warning\n",
    "if device.type == 'cpu':\n",
    "    print(\"⚠️  WARNING: Running on CPU. Expect slower performance!\")\n",
    "    print(\"   - Image generation: ~10-30 seconds per image\")\n",
    "    print(\"   - Projection: ~20-60 minutes\")\n",
    "    print(\"   - Consider using Google Colab with GPU for better performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Pre-trained Model\n",
    "\n",
    "### Download StyleGAN2 model for faces (256x256 version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pre-trained StyleGAN2-FFHQ model (256x256 for faster processing)\n",
    "model_url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl'\n",
    "model_path = 'ffhq.pkl'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Downloading StyleGAN2-FFHQ model...\")\n",
    "    response = requests.get(model_url)\n",
    "    with open(model_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(\"Model downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model and configure for 256x256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the generator\n",
    "with open(model_path, 'rb') as f:\n",
    "    G = pickle.load(f)['G_ema'].to(device)\n",
    "\n",
    "# Force 256x256 output for faster processing\n",
    "original_resolution = G.img_resolution\n",
    "target_resolution = 256\n",
    "\n",
    "print(f\"Original model resolution: {original_resolution}x{original_resolution}\")\n",
    "print(f\"Using resolution: {target_resolution}x{target_resolution}\")\n",
    "print(f\"Z latent space dimension: {G.z_dim}\")\n",
    "print(f\"W latent space dimension: {G.w_dim}\")\n",
    "\n",
    "# Note: We'll resize outputs to 256x256 for faster processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Concepts**:\n",
    "\n",
    "- **Z Space**: Original latent space (Gaussian)\n",
    "- **W Space**: Intermediate latent space (more disentangled)\n",
    "- **StyleGAN2 vs StyleGAN1**: Better quality, fewer artifacts, improved architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic Image Generation\n",
    "\n",
    "### Helper function to display images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_pil(tensor, target_size=256):\n",
    "    \"\"\"Convert tensor to PIL image\"\"\"\n",
    "    tensor = (tensor + 1) * 127.5  # From [-1,1] to [0,255]\n",
    "    tensor = tensor.clamp(0, 255).to(torch.uint8)\n",
    "    tensor = tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    img = Image.fromarray(tensor)\n",
    "    \n",
    "    # Resize for consistent display\n",
    "    if img.size != (target_size, target_size):\n",
    "        img = img.resize((target_size, target_size), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def show_images(images, titles=None, figsize=(15, 5)):\n",
    "    \"\"\"Display multiple images\"\"\"\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(1, n, figsize=figsize)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (img, ax) in enumerate(zip(images, axes)):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random vectors in Z space\n",
    "num_samples = 4\n",
    "z = torch.randn([num_samples, G.z_dim]).to(device)\n",
    "\n",
    "print(\"Generating random faces...\")\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    # Map from Z to W\n",
    "    w = G.mapping(z, None)\n",
    "    # Generate images\n",
    "    imgs = G.synthesis(w)\n",
    "\n",
    "# Convert and display\n",
    "pil_images = [tensor_to_pil(img) for img in imgs]\n",
    "show_images(pil_images, titles=[f'Image {i+1}' for i in range(num_samples)])\n",
    "print(\"✅ Generation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection Question**: Why does StyleGAN use two latent spaces (Z and W) instead of just one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real Image Projection to Latent Space\n",
    "\n",
    "### Load input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path_or_upload=True, target_size=256):\n",
    "    \"\"\"Load image from file or upload\"\"\"\n",
    "    if image_path_or_upload == True:\n",
    "        # Option 1: Upload image\n",
    "        print(\"Upload your image (preferably a face, any size):\")\n",
    "        uploaded = files.upload()\n",
    "        image_path = list(uploaded.keys())[0]\n",
    "    else:\n",
    "        # Option 2: Use example image\n",
    "        # Download example image\n",
    "        example_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/5/50/Vd-Orig.png/256px-Vd-Orig.png\"\n",
    "        response = requests.get(example_url)\n",
    "        image_path = \"example_face.jpg\"\n",
    "        with open(image_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # Load and process image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image.resize((target_size, target_size), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1).float()\n",
    "    image_tensor = (image_tensor / 127.5) - 1  # Normalize to [-1, 1]\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    return image_tensor, image\n",
    "\n",
    "# Load image (change to False to use example)\n",
    "target_tensor, target_pil = load_image(True)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(target_pil)\n",
    "plt.title(\"Target Image\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection function (inversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_image(G, target, num_steps=300, lr=0.01):\n",
    "    \"\"\"Project an image to latent space W\"\"\"\n",
    "    # Initialize w latent\n",
    "    w_avg = G.mapping.w_avg.unsqueeze(0).unsqueeze(1).repeat([1, G.mapping.num_ws, 1])\n",
    "    w = w_avg.clone().detach().requires_grad_(True)\n",
    "  \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam([w], lr=lr)\n",
    "  \n",
    "    # Loss function\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "  \n",
    "    losses = []\n",
    "  \n",
    "    print(f\"Starting projection for {num_steps} steps...\")\n",
    "    for step in range(num_steps):\n",
    "        # Forward pass\n",
    "        synth_img = G.synthesis(w)\n",
    "      \n",
    "        # Resize target to match synthesis output if needed\n",
    "        if synth_img.shape != target.shape:\n",
    "            target_resized = F.interpolate(target, size=synth_img.shape[2:], mode='bilinear', align_corners=False)\n",
    "        else:\n",
    "            target_resized = target\n",
    "      \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(synth_img, target_resized)\n",
    "      \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "        losses.append(loss.item())\n",
    "      \n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
    "  \n",
    "    print(\"✅ Projection complete!\")\n",
    "    return w.detach(), losses\n",
    "\n",
    "# Project the image (reduced steps for CPU compatibility)\n",
    "cpu_steps = 300 if device.type == 'cpu' else 500\n",
    "print(f\"Projecting image to latent space ({cpu_steps} steps)...\")\n",
    "if device.type == 'cpu':\n",
    "    print(\"⏰ This may take 20-60 minutes on CPU...\")\n",
    "\n",
    "projected_w, losses = project_image(G, target_tensor, num_steps=cpu_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare projection result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate projected image\n",
    "with torch.no_grad():\n",
    "    projected_img = G.synthesis(projected_w)\n",
    "\n",
    "# Show comparison\n",
    "original = tensor_to_pil(target_tensor[0])\n",
    "reconstructed = tensor_to_pil(projected_img[0])\n",
    "\n",
    "show_images([original, reconstructed], \n",
    "           titles=['Original', 'Reconstructed'], \n",
    "           figsize=(10, 5))\n",
    "\n",
    "# Show loss curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.title('Loss during projection')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Latent Space Exploration\n",
    "\n",
    "### Generate semantic directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-calculated semantic directions for FFHQ\n",
    "# These are approximate directions in W space\n",
    "def get_semantic_directions():\n",
    "    \"\"\"Return basic semantic directions\"\"\"\n",
    "    # Note: These are simplified directions for the example\n",
    "    # In a real project, you'd use directions calculated with methods like SeFa or InterFaceGAN\n",
    "    \n",
    "    directions = {}\n",
    "    \n",
    "    # Generate random samples to find approximate directions\n",
    "    num_samples = 50  # Reduced for CPU\n",
    "    z_samples = torch.randn([num_samples, G.z_dim]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w_samples = G.mapping(z_samples, None)\n",
    "    \n",
    "    # Calculate mean direction (simplified)\n",
    "    w_mean = w_samples.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    # Directions based on simplified PCA\n",
    "    # In practice, you'd use more precise pre-calculated directions\n",
    "    directions['age'] = torch.randn_like(w_mean) * 0.1\n",
    "    directions['smile'] = torch.randn_like(w_mean) * 0.1\n",
    "    directions['gender'] = torch.randn_like(w_mean) * 0.1\n",
    "    \n",
    "    return directions\n",
    "\n",
    "print(\"Loading semantic directions...\")\n",
    "semantic_directions = get_semantic_directions()\n",
    "print(\"✅ Semantic directions loaded:\", list(semantic_directions.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to apply edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_edit(w, direction, strength=1.0):\n",
    "    \"\"\"Apply a semantic edit to a latent vector\"\"\"\n",
    "    return w + direction * strength\n",
    "\n",
    "def show_edit_progression(w_base, direction, strengths=[-2, -1, 0, 1, 2], title=\"Edit\"):\n",
    "    \"\"\"Show progression of an edit\"\"\"\n",
    "    images = []\n",
    "    titles = []\n",
    "  \n",
    "    print(f\"Generating {title} progression...\")\n",
    "    for strength in strengths:\n",
    "        w_edited = apply_edit(w_base, direction, strength)\n",
    "      \n",
    "        with torch.no_grad():\n",
    "            img = G.synthesis(w_edited)\n",
    "      \n",
    "        images.append(tensor_to_pil(img[0]))\n",
    "        titles.append(f'{title}: {strength}')\n",
    "  \n",
    "    show_images(images, titles, figsize=(20, 4))\n",
    "    print(\"✅ Edit progression complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Directed Image Editing\n",
    "### Apply different edits to your projected image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the previously projected image\n",
    "base_w = projected_w\n",
    "\n",
    "# Apply \"age\" edit\n",
    "print(\"=== Edit: Age ===\")\n",
    "show_edit_progression(base_w, semantic_directions['age'], \n",
    "                     strengths=[-1.5, -0.75, 0, 0.75, 1.5], \n",
    "                     title=\"Age\")\n",
    "\n",
    "# Apply \"smile\" edit\n",
    "print(\"\\n=== Edit: Smile ===\")\n",
    "show_edit_progression(base_w, semantic_directions['smile'], \n",
    "                     strengths=[-1.5, -0.75, 0, 0.75, 1.5], \n",
    "                     title=\"Smile\")\n",
    "\n",
    "# Apply \"gender\" edit\n",
    "print(\"\\n=== Edit: Gender ===\")\n",
    "show_edit_progression(base_w, semantic_directions['gender'], \n",
    "                     strengths=[-1.5, -0.75, 0, 0.75, 1.5], \n",
    "                     title=\"Gender\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of multiple edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_edits(w_base, edits_dict):\n",
    "    \"\"\"Combine multiple edits\"\"\"\n",
    "    w_edited = w_base.clone()\n",
    "  \n",
    "    for direction_name, strength in edits_dict.items():\n",
    "        if direction_name in semantic_directions:\n",
    "            w_edited = apply_edit(w_edited, semantic_directions[direction_name], strength)\n",
    "  \n",
    "    return w_edited\n",
    "\n",
    "# Example: Combine multiple edits\n",
    "edit_combinations = [\n",
    "    {'age': 0, 'smile': 0, 'gender': 0},  # Original\n",
    "    {'age': 1, 'smile': 0.5, 'gender': 0},  # Older and smiling\n",
    "    {'age': -1, 'smile': -0.5, 'gender': 0.5},  # Younger, less smile\n",
    "    {'age': 0.5, 'smile': 1, 'gender': -0.5},  # Custom combination\n",
    "]\n",
    "\n",
    "images = []\n",
    "titles = []\n",
    "\n",
    "print(\"Generating edit combinations...\")\n",
    "for i, edits in enumerate(edit_combinations):\n",
    "    w_combined = combine_edits(base_w, edits)\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        img = G.synthesis(w_combined)\n",
    "  \n",
    "    images.append(tensor_to_pil(img[0]))\n",
    "    title = \"Original\" if i == 0 else f\"Combo {i}\"\n",
    "    titles.append(title)\n",
    "\n",
    "show_images(images, titles, figsize=(16, 4))\n",
    "print(\"✅ Edit combinations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Latent Space Interpolation\n",
    "### Interpolation between two faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two random faces\n",
    "z1 = torch.randn([1, G.z_dim]).to(device)\n",
    "z2 = torch.randn([1, G.z_dim]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    w1 = G.mapping(z1, None)\n",
    "    w2 = G.mapping(z2, None)\n",
    "\n",
    "# Interpolation\n",
    "def interpolate_w(w1, w2, num_steps=5):\n",
    "    \"\"\"Interpolate between two w vectors\"\"\"\n",
    "    alphas = np.linspace(0, 1, num_steps)\n",
    "    interpolated = []\n",
    "  \n",
    "    for alpha in alphas:\n",
    "        w_interp = w1 * (1 - alpha) + w2 * alpha\n",
    "        interpolated.append(w_interp)\n",
    "  \n",
    "    return interpolated, alphas\n",
    "\n",
    "w_interpolated, alphas = interpolate_w(w1, w2, num_steps=7)\n",
    "\n",
    "# Generate interpolated images\n",
    "images = []\n",
    "print(\"Generating interpolation sequence...\")\n",
    "for w_interp in w_interpolated:\n",
    "    with torch.no_grad():\n",
    "        img = G.synthesis(w_interp)\n",
    "    images.append(tensor_to_pil(img[0]))\n",
    "\n",
    "titles = [f'α={alpha:.2f}' for alpha in alphas]\n",
    "show_images(images, titles, figsize=(21, 3))\n",
    "print(\"✅ Interpolation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Additional Experiments\n",
    "### Free exploration of latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive function to explore latent space\n",
    "def random_walk_in_latent_space(start_w, num_steps=5, step_size=0.3):\n",
    "    \"\"\"Random walk in latent space\"\"\"\n",
    "    current_w = start_w.clone()\n",
    "    path = [current_w.clone()]\n",
    "  \n",
    "    for _ in range(num_steps):\n",
    "        # Random step\n",
    "        noise = torch.randn_like(current_w) * step_size\n",
    "        current_w = current_w + noise\n",
    "        path.append(current_w.clone())\n",
    "  \n",
    "    return path\n",
    "\n",
    "# Start from your projected image\n",
    "walk_path = random_walk_in_latent_space(projected_w, num_steps=6, step_size=0.2)\n",
    "\n",
    "# Generate images from the walk\n",
    "walk_images = []\n",
    "print(\"Generating random walk sequence...\")\n",
    "for i, w in enumerate(walk_path):\n",
    "    with torch.no_grad():\n",
    "        img = G.synthesis(w)\n",
    "    walk_images.append(tensor_to_pil(img[0]))\n",
    "\n",
    "show_images(walk_images[:4], titles=[f'Step {i}' for i in range(4)], figsize=(16, 4))\n",
    "print(\"✅ Random walk complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your favorite edited image\n",
    "def save_result(image_tensor, filename=\"stylegan_result.png\"):\n",
    "    \"\"\"Save result image\"\"\"\n",
    "    pil_img = tensor_to_pil(image_tensor[0])\n",
    "    pil_img.save(filename)\n",
    "    print(f\"Image saved as: {filename}\")\n",
    "    return pil_img\n",
    "\n",
    "# Example: Save a specific edit\n",
    "favorite_edit = combine_edits(base_w, {'age': 0.5, 'smile': 1.0, 'gender': 0})\n",
    "with torch.no_grad():\n",
    "    favorite_img = G.synthesis(favorite_edit)\n",
    "\n",
    "save_result(favorite_img, \"my_edited_face.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Important Theoretical Concepts\n",
    "\n",
    "### How does StyleGAN work?\n",
    "\n",
    "**Key Architecture:**\n",
    "\n",
    "- **Mapping Network**: Z → W (8 FC layers)\n",
    "- **Synthesis Network**: W → Image (AdaIN at each layer)\n",
    "- **Discriminator**: Judges image realism\n",
    "\n",
    "**Advantages of W Space:**\n",
    "\n",
    "- More **disentangled** (independent features)\n",
    "- Better **linear interpolation**\n",
    "- More **semantic control**\n",
    "\n",
    "### Differences between versions:\n",
    "```python\n",
    "# StyleGAN1 (2019): First version, artifact problems\n",
    "# StyleGAN2 (2020): Improved architecture, fewer artifacts\n",
    "# StyleGAN3 (2021): Rotation and translation invariance\n",
    "\n",
    "print(\"We're using StyleGAN2 - the perfect balance between quality and speed\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Challenges and Exercises\n",
    "### Exercise 1: Experiment with your own directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your own custom semantic direction\n",
    "# Hint: Combine existing directions with different weights\n",
    "\n",
    "custom_direction = (semantic_directions['age'] * 0.5 + \n",
    "                   semantic_directions['smile'] * 0.3)\n",
    "\n",
    "show_edit_progression(base_w, custom_direction, \n",
    "                     strengths=[-2, -1, 0, 1, 2], \n",
    "                     title=\"Custom Direction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Find the \"average face\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple faces and calculate average in W space\n",
    "num_faces = 10\n",
    "z_batch = torch.randn([num_faces, G.z_dim]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    w_batch = G.mapping(z_batch, None)\n",
    "  \n",
    "# Calculate average face\n",
    "w_average = w_batch.mean(dim=0, keepdim=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    avg_face = G.synthesis(w_average)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(tensor_to_pil(avg_face[0]))\n",
    "plt.title(\"Average Face\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Reflections\n",
    "\n",
    "### What we have learned:\n",
    "\n",
    "1. **StyleGAN** generates high-quality images using structured latent spaces\n",
    "2. **Image projection** allows us to edit real photos\n",
    "3. **W space** is more interpretable than Z for semantic edits\n",
    "4. **Semantic directions** enable controlled and predictable edits\n",
    "\n",
    "### Real-world applications:\n",
    "\n",
    "- **Digital art** and creativity\n",
    "- **Advanced photo editing**\n",
    "- **Synthetic dataset** generation\n",
    "- **Research** in visual representation\n",
    "\n",
    "### Ethical considerations:\n",
    "\n",
    "- **Deepfakes** and media manipulation\n",
    "- **Bias** in training data\n",
    "- **Consent** in image usage"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
