{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: LLMs Generativos - Prompt Engineering, LoRA, y Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de dependencias\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Prompt Engineering y Formato de Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Cargar Modelo Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tiiuae/Falcon3-1B-Base\"\n",
    "\n",
    "# Cargar tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Cargar modelo base (sin quantization por ahora)\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Modelo cargado: {model_name}\")\n",
    "print(f\"Parámetros: {model_base.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Función para Generar Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, max_length=200, temperature=0.7):\n",
    "    \"\"\"Genera texto usando el modelo\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "  \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            attention_mask=inputs.attention_mask\n",
    "        )\n",
    "  \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text[len(prompt):]  # Solo la parte generada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Comparación de Prompts: Raw vs Structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pregunta técnica simple\n",
    "question = \"Explica qué es overfitting en machine learning\"\n",
    "\n",
    "print(\"=== PROMPT RAW (Modelo Base) ===\")\n",
    "raw_prompt = question\n",
    "response_raw = generate_text(model_base, raw_prompt)\n",
    "print(f\"Input: {raw_prompt}\")\n",
    "print(f\"Output: {response_raw}\\n\")\n",
    "\n",
    "print(\"=== PROMPT ESTRUCTURADO ===\")\n",
    "structured_prompt = f\"\"\"Pregunta: {question}\n",
    "Respuesta detallada:\"\"\"\n",
    "response_structured = generate_text(model_base, structured_prompt)\n",
    "print(f\"Input: {structured_prompt}\")\n",
    "print(f\"Output: {response_structured}\\n\")\n",
    "\n",
    "print(\"=== FORMATO INSTRUCTION (Como en el entrenamiento) ===\")\n",
    "instruction_prompt = f\"\"\"### Instrucción:\n",
    "{question}\n",
    "\n",
    "### Respuesta:\"\"\"\n",
    "response_instruction = generate_text(model_base, instruction_prompt)\n",
    "print(f\"Input: {instruction_prompt}\")\n",
    "print(f\"Output: {response_instruction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: LoRA Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preparar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset Alpaca\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\")\n",
    "print(f\"Dataset original: {len(dataset['train'])} ejemplos\")\n",
    "\n",
    "# Tomar una muestra pequeña para el lab\n",
    "train_dataset = dataset['train'].select(range(1000))  # Solo 1000 ejemplos\n",
    "print(f\"Dataset para entrenamiento: {len(train_dataset)} ejemplos\")\n",
    "\n",
    "# Ver algunos ejemplos\n",
    "for i in range(3):\n",
    "    example = train_dataset[i]\n",
    "    print(f\"Ejemplo {i+1}:\")\n",
    "    print(f\"Instrucción: {example['instruction'][:100]}...\")\n",
    "    print(f\"Input: {example['input'][:50]}...\" if example['input'] else \"Input: (vacío)\")\n",
    "    print(f\"Output: {example['output'][:100]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Formato del Dataset para SFT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(sample):\n",
    "    \"\"\"Formatear ejemplo en estilo instruction-following\"\"\"\n",
    "    if sample['input']:\n",
    "        prompt = f\"\"\"### Instrucción:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Input:\n",
    "{sample['input']}\n",
    "\n",
    "### Respuesta:\n",
    "{sample['output']}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"### Instrucción:\n",
    "{sample['instruction']}\n",
    "\n",
    "### Respuesta:\n",
    "{sample['output']}\"\"\"\n",
    "  \n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Formatear dataset\n",
    "formatted_dataset = train_dataset.map(format_instruction)\n",
    "print(\"Ejemplo formateado:\")\n",
    "print(formatted_dataset[0]['text'][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Configurar LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración LoRA - ajustada para Falcon\n",
    "lora_config = LoraConfig(\n",
    "    r=8,                          # rank - qué tan 'grande' es la adaptación\n",
    "    lora_alpha=32,               # scaling factor\n",
    "    target_modules=[             # módulos específicos para Falcon\n",
    "        \"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "print(\"Configuración LoRA:\")\n",
    "print(f\"Rank (r): {lora_config.r}\")\n",
    "print(f\"Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"Módulos objetivo: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Preparar Modelo para LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo con LoRA\n",
    "model_lora = get_peft_model(model_base, lora_config)\n",
    "\n",
    "# Estadísticas del modelo\n",
    "trainable_params = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model_lora.parameters())\n",
    "\n",
    "print(f\"Parámetros totales: {total_params:,}\")\n",
    "print(f\"Parámetros entrenables (LoRA): {trainable_params:,}\")\n",
    "print(f\"Porcentaje entrenable: {100 * trainable_params / total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Configurar Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argumentos de entrenamiento - configuración rápida\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-lora-alpaca\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,              # Solo 1 época para el lab\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=25,\n",
    "    save_steps=500,\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=50,\n",
    "    fp16=True,                       # Usar FP16 para velocidad\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "print(\"Configuración de entrenamiento:\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Grad accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Épocas: {training_args.num_train_epochs}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Entrenar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model_lora,\n",
    "    train_dataset=formatted_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"Iniciando entrenamiento LoRA...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Entrenar\n",
    "trainer.train()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Entrenamiento completado en {end_time - start_time:.2f} segundos\")\n",
    "\n",
    "# Guardar adapters LoRA\n",
    "model_lora.save_pretrained(\"./falcon-lora-adapters\")\n",
    "print(\"Adapters LoRA guardados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Comparar Antes/Después del Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preguntas de evaluación\n",
    "test_questions = [\n",
    "    \"¿Qué es machine learning?\",\n",
    "    \"Explica la diferencia entre supervised y unsupervised learning\",\n",
    "    \"¿Cómo funciona un algoritmo de regresión lineal?\",\n",
    "    \"Dame consejos para evitar overfitting\"\n",
    "]\n",
    "\n",
    "print(\"=== COMPARACIÓN: MODELO BASE vs MODELO CON LoRA ===\\n\")\n",
    "\n",
    "for i, question in enumerate(test_questions):\n",
    "    prompt = f\"\"\"### Instrucción:\n",
    "{question}\n",
    "\n",
    "### Respuesta:\"\"\"\n",
    "  \n",
    "    print(f\"PREGUNTA {i+1}: {question}\")\n",
    "    print(\"-\" * 60)\n",
    "  \n",
    "    # Modelo base\n",
    "    print(\"MODELO BASE:\")\n",
    "    response_base = generate_text(model_base, prompt, max_length=150)\n",
    "    print(response_base[:200] + (\"...\" if len(response_base) > 200 else \"\"))\n",
    "    print()\n",
    "  \n",
    "    # Modelo con LoRA\n",
    "    print(\"MODELO + LoRA:\")\n",
    "    response_lora = generate_text(model_lora, prompt, max_length=150)\n",
    "    print(response_lora[:200] + (\"...\" if len(response_lora) > 200 else \"\"))\n",
    "    print(\"=\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Función para Medir Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Obtener uso de memoria GPU y RAM\"\"\"\n",
    "    gpu_memory = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "    ram_memory = psutil.Process().memory_info().rss / 1e9\n",
    "    return gpu_memory, ram_memory\n",
    "\n",
    "def benchmark_model(model, prompt, num_runs=3):\n",
    "    \"\"\"Benchmark de velocidad e inferencia\"\"\"\n",
    "    times = []\n",
    "  \n",
    "    for _ in range(num_runs):\n",
    "        start_time = time.time()\n",
    "        _ = generate_text(model, prompt, max_length=100)\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "  \n",
    "    return sum(times) / len(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Cargar Modelos con Diferentes Quantizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar memoria\n",
    "del model_base\n",
    "del model_lora\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "models = {}\n",
    "memory_usage = {}\n",
    "\n",
    "# 1. Modelo FP16 (baseline)\n",
    "print(\"Cargando modelo FP16...\")\n",
    "models['fp16'] = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "gpu_mem, ram_mem = get_memory_usage()\n",
    "memory_usage['fp16'] = {'gpu': gpu_mem, 'ram': ram_mem}\n",
    "print(f\"Memoria GPU: {gpu_mem:.2f} GB, RAM: {ram_mem:.2f} GB\")\n",
    "\n",
    "# 2. Modelo 8-bit\n",
    "print(\"\\nCargando modelo 8-bit...\")\n",
    "quantization_config_8bit = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "models['8bit'] = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config_8bit,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "gpu_mem, ram_mem = get_memory_usage()\n",
    "memory_usage['8bit'] = {'gpu': gpu_mem, 'ram': ram_mem}\n",
    "print(f\"Memoria GPU: {gpu_mem:.2f} GB, RAM: {ram_mem:.2f} GB\")\n",
    "\n",
    "# 3. Modelo 4-bit\n",
    "print(\"\\nCargando modelo 4-bit...\")\n",
    "quantization_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "models['4bit'] = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config_4bit,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "gpu_mem, ram_mem = get_memory_usage()\n",
    "memory_usage['4bit'] = {'gpu': gpu_mem, 'ram': ram_mem}\n",
    "print(f\"Memoria GPU: {gpu_mem:.2f} GB, RAM: {ram_mem:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Comparar Velocidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== BENCHMARK DE VELOCIDAD ===\")\n",
    "test_prompt = \"Explica qué es deep learning en términos simples:\"\n",
    "\n",
    "benchmark_results = {}\n",
    "for model_type, model in models.items():\n",
    "    print(f\"Benchmarking {model_type}...\")\n",
    "    avg_time = benchmark_model(model, test_prompt)\n",
    "    benchmark_results[model_type] = avg_time\n",
    "    print(f\"Tiempo promedio: {avg_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Comparar Calidad de Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== COMPARACIÓN DE CALIDAD ===\")\n",
    "test_prompt = \"\"\"### Instrucción:\n",
    "Explica la diferencia entre bias y variance en machine learning\n",
    "\n",
    "### Respuesta:\"\"\"\n",
    "\n",
    "for model_type, model in models.items():\n",
    "    print(f\"\\n{model_type.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    response = generate_text(model, test_prompt, max_length=150)\n",
    "    print(response[:300] + (\"...\" if len(response) > 300 else \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Visualizar Comparaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de memoria\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "model_types = list(memory_usage.keys())\n",
    "gpu_memory = [memory_usage[mt]['gpu'] for mt in model_types]\n",
    "plt.bar(model_types, gpu_memory, color=['blue', 'orange', 'green'])\n",
    "plt.title('Uso de Memoria GPU')\n",
    "plt.ylabel('GB')\n",
    "plt.ylim(0, max(gpu_memory) * 1.2)\n",
    "\n",
    "# Gráfico de velocidad\n",
    "plt.subplot(1, 3, 2)\n",
    "speeds = [benchmark_results[mt] for mt in model_types]\n",
    "plt.bar(model_types, speeds, color=['blue', 'orange', 'green'])\n",
    "plt.title('Tiempo de Inferencia')\n",
    "plt.ylabel('Segundos')\n",
    "\n",
    "# Gráfico comparativo\n",
    "plt.subplot(1, 3, 3)\n",
    "reduction_gpu = [(memory_usage['fp16']['gpu'] - memory_usage[mt]['gpu']) / memory_usage['fp16']['gpu'] * 100 \n",
    "                 for mt in model_types]\n",
    "plt.bar(model_types, reduction_gpu, color=['blue', 'orange', 'green'])\n",
    "plt.title('Reducción de Memoria vs FP16')\n",
    "plt.ylabel('% Reducción')\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabla resumen\n",
    "print(\"\\n=== TABLA RESUMEN ===\")\n",
    "df = pd.DataFrame({\n",
    "    'Modelo': model_types,\n",
    "    'GPU Memory (GB)': [memory_usage[mt]['gpu'] for mt in model_types],\n",
    "    'Tiempo Inferencia (s)': [benchmark_results[mt] for mt in model_types],\n",
    "    'Reducción Memoria (%)': reduction_gpu\n",
    "})\n",
    "print(df.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos Adicionales\n",
    "- **Transformers**: https://huggingface.co/docs/transformers/\n",
    "- **PEFT (LoRA)**: https://huggingface.co/docs/peft/\n",
    "- **BitsAndBytesConfig**: https://huggingface.co/docs/transformers/main_classes/quantization\n",
    "- **Alpaca Dataset**: https://huggingface.co/datasets/tatsu-lab/alpaca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar memoria\n",
    "for model in models.values():\n",
    "    del model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Memoria limpiada\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
